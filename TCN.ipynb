{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation_type\n",
      "combustible fuels                        -73747.0\n",
      "hydraulic turbine                             0.0\n",
      "nuclear steam turbine                   -133685.0\n",
      "other types of electricity generation         0.0\n",
      "solar                                         0.0\n",
      "wind power turbine                            0.0\n",
      "dtype: float64\n",
      "Epoch 1/50\n",
      "4/4 - 7s - loss: 84.5059 - mae: 6.7046 - val_loss: 1.0098 - val_mae: 0.7690 - 7s/epoch - 2s/step\n",
      "Epoch 2/50\n",
      "4/4 - 0s - loss: 8.2724 - mae: 2.2160 - val_loss: 1.3958 - val_mae: 0.9647 - 437ms/epoch - 109ms/step\n",
      "Epoch 3/50\n",
      "4/4 - 0s - loss: 5.2441 - mae: 1.7958 - val_loss: 1.1124 - val_mae: 0.8563 - 422ms/epoch - 106ms/step\n",
      "Epoch 4/50\n",
      "4/4 - 0s - loss: 3.7552 - mae: 1.5377 - val_loss: 0.6173 - val_mae: 0.6361 - 382ms/epoch - 96ms/step\n",
      "Epoch 5/50\n",
      "4/4 - 0s - loss: 1.9779 - mae: 1.1160 - val_loss: 0.3266 - val_mae: 0.4623 - 461ms/epoch - 115ms/step\n",
      "Epoch 6/50\n",
      "4/4 - 0s - loss: 1.3973 - mae: 0.9137 - val_loss: 0.1985 - val_mae: 0.3542 - 427ms/epoch - 107ms/step\n",
      "Epoch 7/50\n",
      "4/4 - 0s - loss: 1.2316 - mae: 0.8632 - val_loss: 0.1516 - val_mae: 0.3184 - 346ms/epoch - 86ms/step\n",
      "Epoch 8/50\n",
      "4/4 - 0s - loss: 0.9198 - mae: 0.7686 - val_loss: 0.1359 - val_mae: 0.3051 - 338ms/epoch - 85ms/step\n",
      "Epoch 9/50\n",
      "4/4 - 0s - loss: 0.6911 - mae: 0.6594 - val_loss: 0.1352 - val_mae: 0.3029 - 379ms/epoch - 95ms/step\n",
      "Epoch 10/50\n",
      "4/4 - 0s - loss: 0.6817 - mae: 0.6490 - val_loss: 0.1358 - val_mae: 0.3025 - 361ms/epoch - 90ms/step\n",
      "Epoch 11/50\n",
      "4/4 - 0s - loss: 0.6263 - mae: 0.6313 - val_loss: 0.1341 - val_mae: 0.2987 - 348ms/epoch - 87ms/step\n",
      "Epoch 12/50\n",
      "4/4 - 0s - loss: 0.5519 - mae: 0.5795 - val_loss: 0.1299 - val_mae: 0.2929 - 331ms/epoch - 83ms/step\n",
      "Epoch 13/50\n",
      "4/4 - 0s - loss: 0.5804 - mae: 0.5833 - val_loss: 0.1241 - val_mae: 0.2848 - 333ms/epoch - 83ms/step\n",
      "Epoch 14/50\n",
      "4/4 - 0s - loss: 0.5333 - mae: 0.5738 - val_loss: 0.1171 - val_mae: 0.2757 - 369ms/epoch - 92ms/step\n",
      "Epoch 15/50\n",
      "4/4 - 0s - loss: 0.5316 - mae: 0.5741 - val_loss: 0.1092 - val_mae: 0.2657 - 344ms/epoch - 86ms/step\n",
      "Epoch 16/50\n",
      "4/4 - 0s - loss: 0.5098 - mae: 0.5617 - val_loss: 0.1022 - val_mae: 0.2564 - 329ms/epoch - 82ms/step\n",
      "Epoch 17/50\n",
      "4/4 - 0s - loss: 0.4634 - mae: 0.5362 - val_loss: 0.0962 - val_mae: 0.2479 - 346ms/epoch - 87ms/step\n",
      "Epoch 18/50\n",
      "4/4 - 0s - loss: 0.4860 - mae: 0.5415 - val_loss: 0.0911 - val_mae: 0.2395 - 338ms/epoch - 84ms/step\n",
      "Epoch 19/50\n",
      "4/4 - 0s - loss: 0.3881 - mae: 0.4924 - val_loss: 0.0877 - val_mae: 0.2342 - 329ms/epoch - 82ms/step\n",
      "Epoch 20/50\n",
      "4/4 - 0s - loss: 0.3886 - mae: 0.4927 - val_loss: 0.0844 - val_mae: 0.2286 - 354ms/epoch - 89ms/step\n",
      "Epoch 21/50\n",
      "4/4 - 0s - loss: 0.3706 - mae: 0.4764 - val_loss: 0.0813 - val_mae: 0.2234 - 407ms/epoch - 102ms/step\n",
      "Epoch 22/50\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tcn import TCN\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_and_preprocess_data(filepath):\n",
    "    data = pd.read_csv(filepath)\n",
    "    data['date'] = pd.to_datetime(data['date'])\n",
    "    df_grouped = data.groupby(['date', 'generation_type'], as_index=False)['megawatt_hours'].sum()\n",
    "    return df_grouped\n",
    "\n",
    "# Ensure all generation types are represented for every date\n",
    "def standardize_generation_types(df, unique_types):\n",
    "    all_dates = df['date'].unique()\n",
    "    standardized_rows = [\n",
    "        {'date': date, 'generation_type': g_type, 'megawatt_hours': 0}\n",
    "        for date in all_dates\n",
    "        for g_type in unique_types\n",
    "        if g_type not in df[df['date'] == date]['generation_type'].values\n",
    "    ]\n",
    "    return pd.concat([df, pd.DataFrame(standardized_rows)], ignore_index=True)\n",
    "\n",
    "# Scale the data\n",
    "def scale_data(df):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_data = scaler.fit_transform(df)\n",
    "    return scaled_data, scaler\n",
    "\n",
    "# Create input-output sequences for time-series data\n",
    "def create_sequences(data, n_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - n_steps):\n",
    "        X.append(data[i:i + n_steps])\n",
    "        y.append(data[i + n_steps])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Build the TCN model\n",
    "def build_tcn(input_shape, output_shape):\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        TCN(\n",
    "            nb_filters=64,\n",
    "            kernel_size=3,\n",
    "            nb_stacks=1,\n",
    "            dilations=(1, 2, 4, 8, 16, 20, 32, 64),\n",
    "            padding='causal',\n",
    "            use_skip_connections=True,\n",
    "            dropout_rate=0.25,\n",
    "            return_sequences=False,\n",
    "            activation='relu',\n",
    "            kernel_initializer='he_normal',\n",
    "            use_batch_norm=False,\n",
    "            use_layer_norm=False,\n",
    "            use_weight_norm=False\n",
    "        ),\n",
    "        Dense(output_shape)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Plot forecasted values\n",
    "def plot_forecasts(df_pivot, forecast_df, title):\n",
    "    combined = pd.concat([df_pivot, forecast_df])\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for column in df_pivot.columns:\n",
    "        plt.plot(combined.index, combined[column], label=column)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Energy Generation (MWh)\")\n",
    "    plt.legend(loc='upper left', bbox_to_anchor=(1.05, 1))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Forecast future energy generation\n",
    "def forecast_future(model, data_scaled, scaler, n_steps, forecast_horizon, columns):\n",
    "    forecast_input = data_scaled[-n_steps:].reshape(1, n_steps, data_scaled.shape[1])\n",
    "    forecast = []\n",
    "    for _ in range(len(forecast_horizon)):\n",
    "        prediction = model.predict(forecast_input)\n",
    "        forecast.append(prediction[0])\n",
    "        forecast_input = np.append(forecast_input[:, 1:, :], prediction.reshape(1, 1, -1), axis=1)\n",
    "    forecast_rescaled = scaler.inverse_transform(forecast)\n",
    "    return pd.DataFrame(forecast_rescaled, index=forecast_horizon, columns=columns)\n",
    "\n",
    "# Main Script\n",
    "filepath = r'data\\processed\\quebec_energy.csv'\n",
    "\n",
    "# Load and process data\n",
    "df_grouped = load_and_preprocess_data(filepath)\n",
    "unique_generation_types = df_grouped['generation_type'].unique()\n",
    "df_standardized = standardize_generation_types(df_grouped, unique_generation_types)\n",
    "df_pivot = df_standardized.pivot(index='date', columns='generation_type', values='megawatt_hours').fillna(0)\n",
    "\n",
    "df_pivot[df_pivot < 0] = 0\n",
    "#Check for negative values sum\n",
    "print(df_pivot[df_pivot < 0].sum())\n",
    "\n",
    "\n",
    "# Scale the pivoted data\n",
    "data_scaled, scaler = scale_data(df_pivot)\n",
    "\n",
    "# Define the number of time steps (look-back period)\n",
    "n_steps = 36\n",
    "\n",
    "# Create sequences for training and testing\n",
    "X, y = create_sequences(data_scaled, n_steps)\n",
    "test_size = 24  # Assuming monthly data, 24 months = 2 years\n",
    "train_data, test_data = data_scaled[:-test_size], data_scaled[-test_size - n_steps:]\n",
    "X_train, y_train = create_sequences(train_data, n_steps)\n",
    "X_test, y_test = create_sequences(test_data, n_steps)\n",
    "\n",
    "# Define input and output shapes\n",
    "input_shape = (n_steps, X_train.shape[2])\n",
    "output_shape = y_train.shape[1]\n",
    "\n",
    "# Build and compile the TCN model\n",
    "tcn_model = build_tcn(input_shape, output_shape)\n",
    "\n",
    "# Train the model\n",
    "history = tcn_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test),\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_test = tcn_model.predict(X_test)\n",
    "\n",
    "# Rescale predictions and actual values back to their original scale\n",
    "y_test_rescaled = scaler.inverse_transform(y_test)\n",
    "y_pred_test_rescaled = scaler.inverse_transform(y_pred_test)\n",
    "\n",
    "# Align the lengths for comparison\n",
    "test_dates = df_pivot.index[-test_size:]\n",
    "min_length = min(len(test_dates), len(y_test_rescaled), len(y_pred_test_rescaled))\n",
    "test_dates = test_dates[:min_length]\n",
    "y_test_rescaled = y_test_rescaled[:min_length]\n",
    "y_pred_test_rescaled = y_pred_test_rescaled[:min_length]\n",
    "\n",
    "# Convert to DataFrame for easier handling\n",
    "comparison_df = pd.DataFrame(data=y_test_rescaled, index=test_dates, columns=df_pivot.columns).rename_axis(\"Date\")\n",
    "predicted_df = pd.DataFrame(data=y_pred_test_rescaled, index=test_dates, columns=df_pivot.columns).rename_axis(\"Date\")\n",
    "\n",
    "# Combine actual and predicted values into a single DataFrame for plotting\n",
    "combined_df = pd.concat([comparison_df.add_suffix(' (Actual)'), predicted_df.add_suffix(' (Predicted')], axis=1)\n",
    "\n",
    "# Plot all generation types on the same figure\n",
    "plt.figure(figsize=(16, 8))\n",
    "for generation_type in df_pivot.columns:\n",
    "    plt.plot(combined_df.index, combined_df[f'{generation_type} (Actual)'], label=f'{generation_type} (Actual)', marker='o')\n",
    "    plt.plot(combined_df.index, combined_df[f'{generation_type} (Predicted'], label=f'{generation_type} (Predicted)', linestyle='--')\n",
    "\n",
    "plt.title(\"Predicted vs Actual for All Generation Types\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Energy Generation (MWh)\")\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1.05, 1))\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Retrain the model on the entire dataset\n",
    "X_full, y_full = create_sequences(data_scaled, n_steps)\n",
    "tcn_model.fit(X_full, y_full, epochs=50, batch_size=32, verbose=2)\n",
    "\n",
    "# Forecast future values\n",
    "forecast_horizon = pd.date_range(df_pivot.index[-1] + pd.DateOffset(months=1), '2030-12-01', freq='MS')\n",
    "forecast_df_tcn = forecast_future(tcn_model, data_scaled, scaler, n_steps, forecast_horizon, df_pivot.columns)\n",
    "\n",
    "# Plot the forecast\n",
    "plot_forecasts(df_pivot, forecast_df_tcn, \"Energy Generation Forecast to 2030 (TCN)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AISE4010",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
